{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUcHyDyfmSYX+LUOTVoMr0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[使い方説明はこちらから](https://spectrum-dugong-954.notion.site/XML-18aa1cc6aeb88010b6a6cfad43c690ca?pvs=73)"
      ],
      "metadata": {
        "id": "iLNbOcaVoE-n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1ANkzRJOo7W",
        "outputId": "7c0772c6-c036-43a7-fca5-808556cccbb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mecab-python3\n",
            "  Downloading mecab_python3-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading mecab_python3-1.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.8/588.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=5dc6832ae259a18a25edd2a459a8edc6b3dd848f91d57f8f4bc4340554ccfafb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/fd/e9/ea4459b868e6d2902e8d80e82dbacb6203e05b3b3a58c64966\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite, mecab-python3\n",
            "Successfully installed mecab-python3-1.0.10 unidic-lite-1.0.8\n"
          ]
        }
      ],
      "source": [
        "pip install mecab-python3 unidic-lite"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import collections\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import os\n",
        "import MeCab\n",
        "import csv\n",
        "import glob\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# カウント対象の品詞\n",
        "EXCLUDED_POS_TAGS = [\n",
        "    \"名詞-固有名詞-一般\",\n",
        "    \"名詞-普通名詞-サ変可能\",\n",
        "    \"名詞-普通名詞-形状詞可能\",\n",
        "    \"記号-文字\",\n",
        "    \"名詞-固有名詞-人名-名\",\n",
        "    \"名詞-固有名詞-人名-一般\",\n",
        "    \"形状詞-一般\",\n",
        "    \"名詞-固有名詞-地名-一般\",\n",
        "    \"名詞-普通名詞-一般\",\n",
        "    \"接尾辞-名詞的-一般\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "def words_format(xml_file_path):\n",
        "    \"\"\"XMLファイルからテキストを抽出し、不要なタグ、空白等を削除する関数\"\"\"\n",
        "    try:\n",
        "        tree = ET.parse(xml_file_path)\n",
        "        root = tree.getroot()\n",
        "        text = \"\"\n",
        "\n",
        "        # XMLタグ内のテキストを抽出\n",
        "        for elem in root.iter():\n",
        "            if elem.text:\n",
        "                text += elem.text + \" \"\n",
        "\n",
        "        # 不要な改行や空白を正規化\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"エラーが発生しました：{e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_dict_to_xlsx(data, filename=\"output.xlsx\"):\n",
        "  \"\"\"\n",
        "    辞書データをExcelファイルに保存する関数。\n",
        "\n",
        "    Args:\n",
        "        data (dict): 保存する辞書データ。\n",
        "        filename (str, optional): 保存するファイル名. Defaults to \"output.xlsx\".\n",
        "    \"\"\"\n",
        "  try:\n",
        "    df = pd.DataFrame(list(data.items()), columns=['単語', '回数'])\n",
        "    df.to_excel(filename, index=False, sheet_name='Sheet1')\n",
        "    print(f\"Successfully saved to {filename}\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error saving to file: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_dict_to_txt(data, filename=\"output.txt\"):\n",
        "  \"\"\"\n",
        "  辞書データをCSV形式でtxtファイルに保存する関数。\n",
        "\n",
        "  Args:\n",
        "    data (dict): 保存する辞書データ。\n",
        "    filename (str, optional): 保存するファイル名. Defaults to \"output.txt\".\n",
        "  \"\"\"\n",
        "  try:\n",
        "    with open(filename, 'w', encoding='utf-8', newline='') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(['単語', '回数'])\n",
        "      for word, count in data.items():\n",
        "        writer.writerow([word, count])\n",
        "    print(f\"Successfully saved to {filename}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error saving to file: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sort_dict_by_value_desc(data):\n",
        "  \"\"\"\n",
        "  辞書を値（数値）の降順でソートした新しい辞書を返す関数。\n",
        "\n",
        "  Args:\n",
        "    data (dict): ソートする辞書データ。\n",
        "\n",
        "  Returns:\n",
        "    dict: 値でソートされた新しい辞書（OrderedDict）。\n",
        "  \"\"\"\n",
        "  import collections\n",
        "\n",
        "  sorted_items = sorted(data.items(), key=lambda item: item[1], reverse=True)\n",
        "  return collections.OrderedDict(sorted_items)\n",
        "\n",
        "\n",
        "def group_morphemes_by_pos(data):\n",
        "    \"\"\"\n",
        "    形態素解析の結果を品詞ごとにまとめ、重複を排除する関数。\n",
        "\n",
        "    Args:\n",
        "        data (dict): 形態素解析の結果をまとめた辞書。\n",
        "\n",
        "    Returns:\n",
        "        dict: 品詞をキー、形態素のリストを値とした辞書。\n",
        "    \"\"\"\n",
        "    grouped_morphemes = {}\n",
        "    json_output_test_text = \"\"\n",
        "    count_target_morphemes = [] # カウント対象とするリスト\n",
        "\n",
        "    for key, value in data.items():\n",
        "        morpheme = value[0]\n",
        "        if len(value) > 4:\n",
        "           pos = value[4]\n",
        "        else:\n",
        "            pos = \"\"\n",
        "        json_output_test_text += pos + \"：\" + morpheme + \"\\n\"\n",
        "        if pos in EXCLUDED_POS_TAGS:  # 指定した品詞の場合のみカウント対象に追加\n",
        "            count_target_morphemes.append(morpheme)\n",
        "        if pos in grouped_morphemes:\n",
        "            if morpheme not in grouped_morphemes[pos]:\n",
        "              grouped_morphemes[pos].append(morpheme)\n",
        "        else:\n",
        "            grouped_morphemes[pos] = [morpheme]\n",
        "\n",
        "    save_string_to_txt(json_output_test_text, filename=\"test_pos_morpheme.txt\")\n",
        "    return grouped_morphemes, count_target_morphemes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_to_json(data, filename=\"output.json\"):\n",
        "    \"\"\"\n",
        "    辞書データをJSONファイルとして保存する関数。\n",
        "\n",
        "    Args:\n",
        "        data (dict): 保存する辞書データ。\n",
        "        filename (str, optional): 保存するファイル名. Defaults to \"output.json\".\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "          json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"Successfully saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to file: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_string_to_txt(text, filename=\"output.txt\"):\n",
        "    \"\"\"\n",
        "    文字列変数をテキストファイルに保存する関数。\n",
        "\n",
        "    Args:\n",
        "        text (str): 保存する文字列。\n",
        "        filename (str, optional): 保存するファイル名. Defaults to \"output.txt\".\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "        print(f\"Successfully saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to file: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def string_to_dict(text):\n",
        "    \"\"\"\n",
        "    文字列を連想配列に変換する関数。\n",
        "\n",
        "    Args:\n",
        "        text (str): 変換する文字列。\n",
        "\n",
        "    Returns:\n",
        "        dict: 連想配列 (辞書)。\n",
        "    \"\"\"\n",
        "    result_dict = {}\n",
        "    lines = text.strip().split(\"\\n\")\n",
        "    for index, line in enumerate(lines, start=1):  # start=1 で行番号を1から開始\n",
        "       parts = line.split(\"\\t\")\n",
        "       # 空文字を削除\n",
        "       parts = [part for part in parts if part != \"\"]\n",
        "       result_dict[index] = parts\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "target_dir = '/'  # XMLファイルが格納されているフォルダのパスを設定（カレントディレクトリ）\n",
        "\n",
        "xml_files = glob.glob(os.path.join(target_dir, '*.xml'))\n",
        "\n",
        "if not xml_files:\n",
        "    print(\"xmlファイルが存在しません。\")\n",
        "    exit(1)  # エラー終了\n",
        "elif len(xml_files) > 1:\n",
        "    print(\"xmlファイルが複数存在します。\")\n",
        "    exit(1)  # エラー終了\n",
        "else:\n",
        "    target_file_path = xml_files[0]\n",
        "    print(f\"Processing XML file: {target_file_path}\") # 処理対象のファイルパスを表示\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# MeCabによる形態素解析実行\n",
        "wakati = MeCab.Tagger(\"-Owakati\")\n",
        "output_words = wakati.parse(target_words).split()\n",
        "\n",
        "test_text = \"\"\n",
        "for word in output_words:\n",
        "  test_text += word + \"\\n\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "target_words = words_format(target_file_path)\n",
        "\n",
        "# MeCabによる形態素解析実行\n",
        "normal_mecab = MeCab.Tagger()\n",
        "\n",
        "\n",
        "\n",
        "output_words_dir = string_to_dict(normal_mecab.parse(target_words))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 品詞毎にグループ化して.json出力\n",
        "group_morphemes = group_morphemes_by_pos(output_words_dir)\n",
        "group_morphemes, count_target_morphemes = group_morphemes_by_pos(output_words_dir)\n",
        "save_to_json(group_morphemes, \"grouped_word.json\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 文字数カウントしてファイル×2出力\n",
        "\n",
        "word_counter = {}\n",
        "\n",
        "for word in count_target_morphemes:\n",
        "  if word in word_counter:\n",
        "    word_counter[word] += 1\n",
        "  else:\n",
        "    word_counter[word] = 1\n",
        "\n",
        "word_counter = sort_dict_by_value_desc(word_counter)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "word_counter = sort_dict_by_value_desc(word_counter)\n",
        "\n",
        "save_dict_to_txt(word_counter, \"output.txt\")\n",
        "save_dict_to_xlsx(word_counter, \"output.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOH36qwZO9Xz",
        "outputId": "68ced7b9-8f33-41bb-837e-1e2f7a198166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing XML file: /347CO0000000318_20241118_506CO0000000342.xml\n",
            "Successfully saved to test_mecab_owakati_process_after.txt\n",
            "Successfully saved to test_mecab_process_before.txt\n",
            "Successfully saved to test_not_split.txt\n",
            "Successfully saved to test_pos_morpheme.txt\n",
            "Successfully saved to test_pos_morpheme.txt\n",
            "Successfully saved to grouped_word.json\n",
            "Successfully saved to output.txt\n",
            "Successfully saved to output.xlsx\n"
          ]
        }
      ]
    }
  ]
}